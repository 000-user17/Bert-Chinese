{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt   #jupyter要matplotlib.pyplot\n",
    "from pytorch_transformers import BertConfig, BertTokenizer, BertModel, BertForMaskedLM\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu' \n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('./bert-pretrained-chinese') #必须要./表示当前文件夹的某个文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 1296, 3636, 3351, 2802, 5439, 5988, 102, 872, 1762, 1525, 102]\n"
     ]
    }
   ],
   "source": [
    "text = '[CLS]单武松打老虎[SEP] 你在哪[SEP]'\n",
    "tokens = tokenizer.tokenize(text)\n",
    "indexed_tokens = tokenizer.convert_tokens_to_ids(tokens)\n",
    "segments_ids = [0,0,0,0,0,0,0,0,1,1,1,1]\n",
    "tokens_tensor = torch.tensor([indexed_tokens])\n",
    "segments_tensors = torch.tensor([segments_ids])\n",
    "print(indexed_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 12])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(21128, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = BertModel.from_pretrained('./bert-pretrained-chinese')\n",
    "model #创建bert模型并加再预训练参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(device)\n",
    "tokens_tensor = tokens_tensor.to(device)\n",
    "segments_tensors = segments_tensors.to(device) #放入gpu中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 12, 768])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval() #eval()将我们的模型置于评估模式，而不是训练模式。在这种情况下，评估模式关闭了训练中使用的dropout正则化。\n",
    "with torch.no_grad():\n",
    "    encoded_layers, pooled_output = model(tokens_tensor, segments_tensors)\n",
    "encoded_layers.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "bert模型输入，tokens_tensor：经过字典序后的one-hot编码，segments_tensors:句子编码，输出encoded_layers：encoded_layer：长度为num_hidden_layers的(batch_size， sequence_length，hidden_size)的Tensor.列表\n",
    "即输入的句子的每个字被转化为768维度的embedding\n",
    "\n",
    "encoded_layer：长度为num_hidden_layers的(batch_size， sequence_length，hidden_size)的Tensor.列表\n",
    "pooled_output: (batch_size, hidden_size), 最后一层encoder的第一个词[CLS]经过Linear层和激活函数Tanh()后的Tensor. 其代表了句子信息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6022, 3)\n",
      "(1506, 2)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "train = pd.read_csv('./dataload/train.csv')\n",
    "test = pd.read_csv('./dataload/test.csv')\n",
    "print(train.shape)\n",
    "print(test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1296, 3403, 2769, 3300, 749, 8024, 3297, 6818, 8958, 1352, 3403, 6819, 4385, 3833, 1220, 1962]\n",
      "['单', '标', '我', '有', '了', '，', '最', '近', 'visa', '双', '标', '返', '现', '活', '动', '好']\n"
     ]
    }
   ],
   "source": [
    "text1 = train['text'][1]\n",
    "tokens1 = tokenizer.tokenize(text1)\n",
    "indexed_tokens1 = tokenizer.convert_tokens_to_ids(tokens1)\n",
    "segments_ids1 = [0,0,0,0,0,0,0,0,1,1,1,1]\n",
    "tokens_tensor1 = torch.tensor([indexed_tokens1])\n",
    "segments_tensors1 = torch.tensor([segments_ids1])\n",
    "print(indexed_tokens1)\n",
    "print(tokens1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''利用bert进行对train.csv的每句话分词，并且得到embedding'''\n",
    "#获取文本经过bert处理的embedding vector\n",
    "encoded_layers = []\n",
    "tokens = []\n",
    "for l in range(train.shape[0]):  #train.shape[0]表示有多少行数据\n",
    "    token = tokenizer.tokenize(train['text'][l]) #词性标注任务我没有添加[CLS]和[SEP]\n",
    "    tokens.append(token)\n",
    "\n",
    "    indexed_token = tokenizer.convert_tokens_to_ids(token)\n",
    "    indexed_token_tensor = torch.tensor([indexed_token]).to(device)\n",
    "\n",
    "    segments_id = [1] * len(token)\n",
    "    segments_id_tensor = torch.tensor([segments_id]).to(device)\n",
    "\n",
    "    model.eval() #eval()将我们的模型置于评估模式，而不是训练模式。在这种情况下，评估模式关闭了训练中使用的dropout正则化。\n",
    "    with torch.no_grad():\n",
    "        word_embedding, pooled_output = model(indexed_token_tensor, segments_id_tensor)\n",
    "\n",
    "    encoded_layers.append(word_embedding)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由于bert的tokenizer分词出来会把'visa'单独当成一个token，因此如果要把'visa'分成'v','i','s','a'就要先用one for one将每个字母和字分开，再带入bert中获得embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''利用one for one 对train.csv的每句话进行分词，再带入bert得到embedding'''\n",
    "encoded_layers = []\n",
    "tokens = []\n",
    "for l in range(train.shape[0]):  #train.shape[0]表示有多少行数据\n",
    "    token = [one for one in train['text'][l]]\n",
    "    tokens.append(token)\n",
    "\n",
    "    indexed_token = tokenizer.convert_tokens_to_ids(token)\n",
    "    indexed_token_tensor = torch.tensor([indexed_token]).to(device)\n",
    "\n",
    "    segments_id = [1] * len(token)\n",
    "    segments_id_tensor = torch.tensor([segments_id]).to(device)\n",
    "\n",
    "    model.eval() #eval()将我们的模型置于评估模式，而不是训练模式。在这种情况下，评估模式关闭了训练中使用的dropout正则化。\n",
    "    with torch.no_grad():\n",
    "        word_embedding, pooled_output = model(indexed_token_tensor, segments_id_tensor)\n",
    "\n",
    "    encoded_layers.append(word_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 84, 768])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_layers[0].shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''获取bert的embedding dim'''\n",
    "#batch_size, sequence_len, embedding_size\n",
    "embedding_dim = encoded_layers[0].shape[2]\n",
    "#bert的embedding dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([84])\n"
     ]
    }
   ],
   "source": [
    "'''得到所有的命名实体的字典，然后根据字典得到train.csv每句话的命名实体字典值'''\n",
    "def tag_to_tensor(tags, tag_to_ix):\n",
    "    tag_idx = [tag_to_ix[w] for w in tags]\n",
    "    return torch.tensor(tag_idx)\n",
    "\n",
    "tag_to_ix = {\"B-BANK\": 0, \"I-BANK\": 1, \"B-PRODUCT\": 2, \"I-PRODUCT\": 3, \"O\": 4, \"B-COMMENTS_N\": 5, \"I-COMMENTS_N\": 6, \"B-COMMENTS_ADJ\": 7, \"I-COMMENTS_ADJ\": 8}  # Assign each tag with a unique index\n",
    "\n",
    "tags = [] #tags是一个list，里面存放每个训练数据的文本对应的命名实体转化为字典tensor后的形式\n",
    "for l in range(train.shape[0]):\n",
    "    tag = train['BIO_anno'][l].split()  #将每一个BIO命名实体分离\n",
    "    tag_tensors = tag_to_tensor(tag, tag_to_ix)\n",
    "    tags.append(tag_tensors.to(device))\n",
    "\n",
    "print(tags[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果list中的元素为tensor，就无法用上述语句转换，会报错\n",
    "ValueError: only one element tensors can be converted to Python scalars\n",
    "遇到这种情况，一定要使用numpy形式存储于list中，然后再进行转换。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pytorch LSTM模型架构\n",
    "class LSTMTagger(nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size, tagset_size):\n",
    "        #LSTM的输入为连两个输入：input, (embedding_dim, hidden_dim),但是这里要用word2vec来embedding，所以要有后两个vocab_size等\n",
    "        #输出：output, (hn,cn)，cn为\n",
    "        super(LSTMTagger, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        # The LSTM takes word embeddings as inputs, and outputs hidden states\n",
    "        # with dimensionality hidden_dim.\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim)\n",
    "\n",
    "        # The linear layer that maps from hidden state space to tag space\n",
    "        self.hidden2tag = nn.Linear(hidden_dim, tagset_size)\n",
    "\n",
    "    def forward(self, sentence):\n",
    "        #forward里的参数决定了model的输入\n",
    "        embeds = self.word_embeddings(sentence)\n",
    "        lstm_out, _ = self.lstm(embeds.view(len(sentence), 1, -1))\n",
    "        #LSTM的输出有两个，第一个是LSTM输出的embedding，第二个应该是lstm里的cell参数\n",
    "        tag_space = self.hidden2tag(lstm_out.view(len(sentence), -1))\n",
    "        #在lstm后加入了linear layer的输出，用于后面的softmax选取命名实体\n",
    "        tag_scores = F.log_softmax(tag_space, dim=1)\n",
    "        return tag_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#本文利用bert先获得训练数据embedding，再加LSTM的模型\n",
    "'''\n",
    "embedding_dim: bert输出的每个字的embedding vector长度\n",
    "hidden_dim: LSTM的每层隐藏元个数，即经过LSTM转化后的embedding dim\n",
    "tagset_size:命名实体BIO的总个数，用于分类\n",
    "num_layers:LSTM层数\n",
    "'''\n",
    "class BertLSTM(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, tagset_size, num_layers):  #embedding_dim:bert处理后的字的ebedding size，hidden_dim：LSTM中对字的embedding size，tagset_size:BIO命名实体数量,num_layers:LSTM的层数\n",
    "\n",
    "        super(BertLSTM,self).__init__()\n",
    "\n",
    "        self.hidden_dim = hidden_dim #self\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers, batch_first=False, dropout=0.5, bias=True, bidirectional=True)\n",
    "        #LSTM设置,前两个时必须有的，后面依次时LSTM层数，bias时y=wx+b的b，bidirectional双向LSTM\n",
    "        self.hidden2tag = nn.Linear(hidden_dim, tagset_size)\n",
    "        #线性层,要用softmax选择命名实体\n",
    "\n",
    "    def forward(self, embedding_bert):  #forward表示模型在train中要输入的参数，这里只用输入文本经过bert的embedding表示\n",
    "        hidden_dim = self.hidden_dim\n",
    "\n",
    "        lstm_out, _ = self.lstm(embedding_bert.permute(1,0,2))\n",
    "       #打乱embedding_bert顺序使其符合lstm的输入形式\n",
    "        out = lstm_out[:,:, [i for i in range(hidden_dim)]]\n",
    "        #取双向lstm输入的前一半部分,不能lstm.out.shape[2]/2，会改变为float类型\n",
    "        tag_space = self.hidden2tag(out.view(embedding_bert.shape[1], -1))\n",
    "\n",
    "        tag_scores = F.log_softmax(tag_space, dim=1)\n",
    "\n",
    "        return tag_scores\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.5372, -0.6486,  0.8605, -0.0524]],\n",
      "\n",
      "        [[ 0.3100,  0.6123, -0.2303, -0.5068]],\n",
      "\n",
      "        [[ 0.3607,  0.1656, -0.0919, -1.0255]]], dtype=torch.float64)\n",
      "tensor([[[-0.5372, -0.6486]],\n",
      "\n",
      "        [[ 0.3100,  0.6123]],\n",
      "\n",
      "        [[ 0.3607,  0.1656]]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "a = np.random.randn(3,1,4)\n",
    "b = torch.tensor(a)\n",
    "c = b[:, :, [i for i in range(2)]]\n",
    "c.shape\n",
    "print(b)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "模拟双向lstm取前一半的值作为输出"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTM 的输入是三维，其顺序是 (seq_len, batch, input_size),因此在传进去的时候要注意是否匹配\n",
    "seq_len:句子有几个token\n",
    "input_size:x的特征维度\n",
    "x.permute变换tensor维度\n",
    "\n",
    "lstm输出数据：\n",
    "output： 维度和输入数据类似，只不过最后的feature部分会有点不同，即 (seq_len, batch, num_directions * hidden_size)\n",
    "\n",
    "如：\n",
    "x = torch.randn(2, 3, 5)\n",
    "print(x.size())\n",
    "print(x.permute(2, 0, 1).size())\n",
    "\n",
    ">>>torch.Size([2, 3, 5])\n",
    ">>>torch.Size([5, 2, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''定义LSTM模型'''\n",
    "model_LSTM = BertLSTM(embedding_dim, 256, len(tag_to_ix), 3).to(device)\n",
    "loss_function = nn.NLLLoss() #和log softmax搭配使用\n",
    "optimizer = optim.SGD(model_LSTM.parameters(), lr=0.01)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 19, 768])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tags[0].shape\n",
    "encoded_layers[0].shape\n",
    "tags[1].shape\n",
    "encoded_layers[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''定义训练函数'''\n",
    "def train_LSTM(model, loss_func, optimizer, tags, encoded_layers, epochs):\n",
    "    losses = []\n",
    "    iter = []\n",
    "    for epoch in range(epochs):\n",
    "        loss_sum=0\n",
    "        for l in range(len(encoded_layers)):\n",
    "            if encoded_layers[l].shape[1] != len(tags[l]):\n",
    "                continue\n",
    "            #如果命名实体数量和token数量不同，跳过本句\n",
    "            model.zero_grad() #新batch训练时将梯度归0，防止梯度累积\n",
    "            tag_scores = model(encoded_layers[l])\n",
    "            loss = loss_func(tag_scores, tags[l])\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            loss_sum+=loss.item()\n",
    "            \n",
    "        losses.append(loss_sum)\n",
    "        iter.append(epoch)\n",
    "        print(\"the loss of\"+ str(epoch) + \"is\" + str(loss_sum))\n",
    "    \n",
    "    plt.title(\"loss of epoch per————\"+str(loss_func)+ \",\"+ str(epochs)+ \"epochs\")\n",
    "    plt.xlabel(\"loss per 1\")\n",
    "    plt.ylabel(\"LOSS\")\n",
    "    plt.plot(iter, losses)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84\n",
      "84\n",
      "torch.Size([84])\n"
     ]
    }
   ],
   "source": [
    "encoded_layers[0].shape[1] != tags[0].shape\n",
    "print(encoded_layers[0].shape[1])\n",
    "print(len(tags[0]))\n",
    "print(tags[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 84, 768])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_layers[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the loss of0is2884.914151276462\n",
      "the loss of1is2534.033711817581\n",
      "the loss of2is2248.3375493022613\n",
      "the loss of3is2003.7740695099346\n",
      "the loss of4is1827.4039800853934\n",
      "the loss of5is1655.067159080645\n",
      "the loss of6is1531.0094499249244\n",
      "the loss of7is1419.7208306726534\n",
      "the loss of8is1308.4929540661396\n",
      "the loss of9is1220.6540562426671\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_LSTM(model_LSTM, loss_function, optimizer, tags, encoded_layers, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''把所有的token的bert embedding放入LSTM模型中进行一次正向传播，此时模型和数据都还在device对应的设备上'''\n",
    "model_LSTM.eval()\n",
    "tag_scores_all = []\n",
    "with torch.no_grad():\n",
    "    for l in range(train.shape[0]):\n",
    "        temp = model_LSTM(encoded_layers[l])#前向传播一次获得每句话每个字对应的命名实体BIO的softmax分数tensor\n",
    "        tag_scores_all.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.5630, -3.1421, -5.0234, -8.0535, -0.9799, -6.5956, -7.7917, -5.9795,\n",
       "        -7.1931], device='cuda:0')"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag_scores_all[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''把train中所有句子的BIO命名实体放入BIO_all中'''\n",
    "BIO_all=[]\n",
    "for i in range(len(tag_scores_all)):#对于每一句\n",
    "    max_indexs = []\n",
    "    BIO = []\n",
    "    for j in range(tag_scores_all[i].shape[0]):#对于每一句的每个token\n",
    "        tag_scores = tag_scores_all[i]\n",
    "        tag_score_list = list(tag_scores[j])\n",
    "\n",
    "        max_val = max(tag_scores[j])\n",
    "        max_index = tag_score_list.index(max_val)\n",
    "\n",
    "        max_indexs.append(max_index)  #一句话的对应的最大索引\n",
    "    for index in max_indexs:\n",
    "        BIO.append(list(tag_to_ix.keys())[index])#把tag_to_ix的键（key）转化为list，然后找到index对应位置的key\n",
    "    \n",
    "    BIO_all.append(BIO)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "51aeaaa6daebc08988689763a3ac20414fe81410c82edd7fb19e44dddcd7fa9b"
  },
  "kernelspec": {
   "display_name": "Python 3.7.0 ('py3.7torch1.4')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
